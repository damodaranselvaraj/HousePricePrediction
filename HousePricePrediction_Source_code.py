# -*- coding: utf-8 -*-
"""HousePrice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rsqTs7PeWNGPfIeWZAlvw6FSJRA7m4S4
"""

# Import the necessary libraries

# Data handling libraries
import pandas as pd          # For loading and manipulating tabular data
import numpy as np           # For numerical operations

# Visualization libraries
import matplotlib.pyplot as plt
import seaborn as sns

# Import Scikit-Learn modules for preprocessing and modeling
from sklearn.model_selection import train_test_split   # Train-test split
from sklearn.impute import SimpleImputer                # Handle missing values
from sklearn.preprocessing import OneHotEncoder         # Encode categorical data
from sklearn.preprocessing import StandardScaler        # Feature scaling
from sklearn.linear_model import LinearRegression        # Linear Regression model
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score # Error Metrics

# Suppress warnings
import warnings
warnings.filterwarnings("ignore")

# Load housing dataset from CSV file. It is loaded as Dataframe
data = pd.read_csv('/content/sample_data/Housing.csv')

# Check the shape of the dataset (Rows, Columns)
print(f"Dataset Shape: {data.shape}")

# View basic information about columns (Non-null counts, Dtypes)
data.info()

# This will display all the Columns in the dataframe
data.columns

# Preview the first few rows in the dataframe
data.head(2)

# Preview the last few rows in the dataframe
data.tail(2)

# To find the data type of the price column
data.dtypes['price']

# To find the mean, median, Standard Devitation of a Column using the Numpy library
print(f"Mean of area column : {np.mean(data['area'])}")
print(f"Median of area column : {np.median(data['area'])}")
print(f"Standard Devitation of area column : {np.std(data['area'])}")

# There is no function called mode in numpy. As mode is mainly used for Caterogrical columns to find the most repetative value.
#np.mode(data['guestroom'])

# Check statistical summary for numerical columns
data.describe()

# Check summary for categorical columns (unique values, frequency)
data.describe(include="object")

# Check for null values across columns
data.isnull().sum()

# Remove duplicates if any exist
data.duplicated().sum()
data = data.drop_duplicates()

# For numerical columns, fill missing values with the Mean
data["area"] = data["area"].fillna(data["area"].mean())
data.isna().sum()

# For categorical columns, fill missing with the Mode (most frequent)
# .mode()[0] grabs the first mode value found
data["airconditioning"] = data["airconditioning"].fillna(data["airconditioning"].mode()[0])
data.isna().sum()

# You can drop columns if it is not required. Coumns like Unique ID, Role number etc.
# There is not unique id column in this dataset.
#data = data.drop(columns=['column_name'], errors='ignore')
#data.columns

# To see all the unique values in a particular column
data["bedrooms"].unique()

# To find number of unique values in all the Columns.
data.nunique()

# To check the Correlation between the numerical columns
data.corr(numeric_only=True)

# 1. Histogram - Helps to find Skewness and Outliers
plt.hist(data['price'], bins=30)
plt.xlabel("House Price")
plt.ylabel("Frequency")
plt.title("Distribution of House Prices")
plt.show()

# 2. Boxplot (Good for spotting outliers)
plt.boxplot(data['price'])
plt.ylabel("House Price")
plt.title("Boxplot of House Prices")
plt.show()

# Histogram for all the numerical values
data[num_cols].hist(figsize=(12,8))
plt.suptitle("Numerical Feature Distributions")
plt.show()

# Scaterred plot for all the numerical colomns
for col in num_cols:
    plt.scatter(data[col], data['price'])
    plt.xlabel(col)
    plt.ylabel("Price")
    plt.title(f"{col} vs Price")
    plt.show()

# Separate columns into Numerical and Categorical lists
cat_cols = data.select_dtypes(include='object').columns
num_cols = data.select_dtypes(exclude='object').columns

# Calculate correlation matrix
corr_matrix = data[num_cols].corr()

# Plot Heatmap
plt.figure(figsize=(10,6))
sns.heatmap(corr_matrix, cmap='coolwarm', annot=True)
plt.title("Numeric Feature Correlation Heatmap")
plt.show()

#Spliting the Columns into X and y. Independent columns into X and dependent column into y.
# price is the target value.
X = data.drop(columns=['price'], axis=1)
y = data['price']

# Split data into training and testing sets.
# Split: 80% Training, 20% Testing
# random_state ensures reproducibility
train_X,test_X,train_y,test_y = train_test_split(X,y,test_size=0.2,random_state=42)

# Check the shape of the each cateory
train_X.shape,test_X.shape,train_y.shape,test_y.shape

train_cat_cols = train_X.select_dtypes(include='object').columns
train_num_cols = train_X.select_dtypes(exclude='object').columns

# Handle Numerical Columns

# Impute missing values using mean
num_imputer = SimpleImputer(strategy='mean')
train_X_num = num_imputer.fit_transform(train_X[train_num_cols])
test_X_num = num_imputer.transform(test_X[train_num_cols])

# Scale numerical features
scaler = StandardScaler()
train_X_num_scaled = scaler.fit_transform(train_X_num)
test_X_num_scaled = scaler.transform(test_X_num)

# Categorical Features

# Impute missing categorical values using most frequent value
cat_imputer = SimpleImputer(strategy='most_frequent')
train_X_cat = cat_imputer.fit_transform(train_X[train_cat_cols])
test_X_cat = cat_imputer.transform(test_X[train_cat_cols])

# One-hot encode categorical features
# handle_unknown='ignore' allows the model to handle new categories in test data gracefully
encoder = OneHotEncoder(handle_unknown="ignore", sparse_output=False)
train_X_cat_enc = encoder.fit_transform(train_X_cat)
test_X_cat_enc  = encoder.transform(test_X_cat)

# Concatenate processed features back together
train_X_final = np.concatenate([train_X_num_scaled, train_X_cat_enc], axis=1)
test_X_final = np.concatenate([test_X_num_scaled, test_X_cat_enc], axis=1)

# Initialize the Linear Regression model
linear_model = LinearRegression()

# Train the model using the processed training data
linear_model.fit(train_X_final, train_y)

# Generate predictions on the test set
linear_predict = linear_model.predict(test_X_final)

# Calculate Metrics
mae = mean_absolute_error(test_y, linear_predict)
mse = mean_squared_error(test_y, linear_predict)
rmse = np.sqrt(mse)
r2 = r2_score(test_y, linear_predict)

print("Mean Absolute Error:", mae)
print("Mean Squared Error:", mse)
print("Root Mean Squared Error:", rmse)
print("R-squared:", r2)

# Draw a scatter plot for y and predicted y. Display Actaul and predicted in different colors
plt.figure(figsize=(10,6))
plt.scatter(test_y, linear_predict)
plt.xlabel("Actual Price")
plt.ylabel("Predicted Price")
plt.title("Actual vs Predicted Prices")
plt.show()

# Plot residuals (Actual - Predicted). A good model should have residuals randomly scattered around zero.

residuals = test_y - linear_predict

plt.scatter(linear_predict, residuals)
plt.axhline(0)
plt.xlabel("Predicted Price")
plt.ylabel("Residuals")
plt.title("Residuals vs Predicted")
plt.show()

linear_model.coef_

linear_model.intercept_

"""# Linear Regression Equation:
**Y = mX + b**

**m (Slope / Coefficient / Weight)** → how much Y changes when X changes (line rotates/moves)

**b (Intercept / Bias / Constant)** → value of Y when X = 0 (where the line cuts the Y-axis)

**X → **Input / Predictive / Independent variable

**Y →** Output / Response / Dependent variable

*Another representation (Y = X * coef_ + intercept_)*
"""